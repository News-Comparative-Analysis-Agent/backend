import pandas as pd
import os
from dotenv import load_dotenv
import numpy as np
from bertopic import BERTopic
from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer
from sklearn.metrics.pairwise import cosine_similarity
from hdbscan import HDBSCAN
from umap import UMAP
import google.generativeai as genai
import time
import re
import json
from itertools import combinations
from collections import Counter
from konlpy.tag import Okt
# ==========================================
# [ì„¤ì •] API í‚¤ ë° í™˜ê²½ ì„¤ì •
# ==========================================
# Load .env from backend root
BASE_DIR = os.path.dirname(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))
load_dotenv(os.path.join(BASE_DIR, ".env"))

GOOGLE_API_KEY = os.getenv("GOOGLE_API_KEY") # .env íŒŒì¼ì—ì„œ ë¡œë“œ
genai.configure(api_key=GOOGLE_API_KEY)

# ==========================================
# 1. ìœ í‹¸ë¦¬í‹° í•¨ìˆ˜ (ì¤‘ë³µì œê±°, í† í¬ë‚˜ì´ì €)
# ==========================================
def remove_duplicates_fast(df, threshold=0.90):
    if df.empty: return df
    df = df.reset_index(drop=True)
    
    print(f"ğŸ§¹ ì¤‘ë³µ ì œê±° ì „: {len(df)}ê°œ")
    tfidf = TfidfVectorizer(max_features=1000).fit_transform(
        df['content'].str[:300].fillna('')
    )
    duplicates = set()
    batch_size = 500
    num_docs = len(df)
    
    for i in range(0, num_docs, batch_size):
        batch_end = min(i + batch_size, num_docs)
        similarities = cosine_similarity(tfidf[i:batch_end], tfidf)
        for local_idx in range(batch_end - i):
            global_idx = i + local_idx
            if global_idx in duplicates: continue
            target_indices = np.where(similarities[local_idx, global_idx+1:] > threshold)[0]
            duplicates.update(target_indices + (global_idx + 1))
            
    df_clean = df.drop(index=list(duplicates)).reset_index(drop=True)
    print(f"âœ¨ ì¤‘ë³µ ì œê±° ì™„ë£Œ: {len(df_clean)}ê°œ")
    return df_clean

def simple_tokenizer(text):
    """ 
    Okt í˜•íƒœì†Œ ë¶„ì„ê¸°ë¥¼ ì‚¬ìš©í•˜ì—¬ 'ëª…ì‚¬'ë§Œ ì¶”ì¶œí•©ë‹ˆë‹¤. 
    'ë‹¨ì‹ì„' -> 'ë‹¨ì‹', 'ëŒ€í‘œëŠ”' -> 'ëŒ€í‘œ' ë¡œ ê¹”ë”í•˜ê²Œ ë³€í™˜ë©ë‹ˆë‹¤.
    """
    okt = Okt()
    
    # 1. ë¶ˆìš©ì–´ ë¦¬ìŠ¤íŠ¸ (ê³„ì† ì¶”ê°€í•´ì„œ ê´€ë¦¬í•˜ë©´ ì¢‹ìŠµë‹ˆë‹¤)
    stopwords = [
        'ë‰´ìŠ¤', 'ì¢…í•©', 'ì†ë³´', 'ê¸°ì', 'íŠ¹íŒŒì›', 'ìœ„í•´', 'ë°í˜”ë‹¤', 'ëŒ€í•´', 'ê´€ë ¨', 
        'ì˜¤ëŠ˜', 'ì˜¤í›„', 'ì˜¤ì „', 'ê²ƒìœ¼ë¡œ', 'ë”°ë¥´ë©´', 'ìˆëŠ”', 'í–ˆë‹¤', 'ë§í–ˆë‹¤',
        'ë¯¼ì£¼ë‹¹', 'êµ­ë¯¼ì˜í˜', 'ì˜ì›', 'ëŒ€í†µë ¹', 'ëŒ€í‘œ', 'ë¬´ë‹¨ì „ì¬', 'ë°°í¬', 'ê¸ˆì§€',
        'ì´ë‚ ', 'ì–´ì œ', 'ë‚´ì¼', 'ì´ë²ˆ', 'ì§€ë‚œ', 'ê°€ì¥', 'í†µí•´', 'ë•Œë¬¸', 'ê²½ìš°', 
        'ì •ë„', 'ì‚¬ì‹¤', 'ë‚´ìš©', 'ëª¨ë‘', 'ìš°ë¦¬', 'ìì‹ ', 'ë¬¸ì œ', 'ìƒê°', 'ì‚¬ëŒ',
        'ê·¸', 'ì´', 'ì €', 'ìˆ˜', 'ê²ƒ', 'ë“±', 'ì•ˆ', 'ì „', 'í›„', 'ì•½', 'ì¤‘'
    ]
    
    # 2. ëª…ì‚¬ ì¶”ì¶œ (nouns í•¨ìˆ˜ ì‚¬ìš©)
    nouns = okt.nouns(str(text))
    
    # 3. ë¶ˆìš©ì–´ ì œê±° ë° 2ê¸€ì ì´ìƒë§Œ ì„ íƒ
    # (ë‹¨, 'ë‹¹'(Party), 'ë²•'(Law) ì²˜ëŸ¼ 1ê¸€ìì—¬ë„ ì¤‘ìš”í•œ ê±´ ì‚´ë ¤ì•¼ í•¨ -> ì¼ë‹¨ì€ 2ê¸€ì ì´ìƒìœ¼ë¡œ í•„í„°ë§)
    filtered_nouns = [n for n in nouns if n not in stopwords and len(n) >= 2]
    
    return filtered_nouns

# ==========================================
# 2. [NEW] í‚¤ì›Œë“œ ë„¤íŠ¸ì›Œí¬ ë¶„ì„ í•¨ìˆ˜
# ==========================================
def extract_issue_network(texts, top_n_nodes=20, top_n_edges=30):
    """
    íŠ¹ì • ì´ìŠˆì— ì†í•œ ê¸°ì‚¬ í…ìŠ¤íŠ¸ë“¤ì„ ë°›ì•„ 'í‚¤ì›Œë“œ ë„¤íŠ¸ì›Œí¬ JSON'ì„ ìƒì„±í•©ë‹ˆë‹¤.
    """
    edges = []
    node_counter = Counter()

    for text in texts:
        # ê¸°ì‚¬ í•˜ë‚˜ì—ì„œ ë‹¨ì–´ ì¶”ì¶œ (ì¤‘ë³µ ì œê±°í•˜ì—¬ ê´€ê³„ ìƒì„±)
        tokens = list(set(simple_tokenizer(text)))
        node_counter.update(tokens)
        
        # ë™ì‹œ ì¶œí˜„(Co-occurrence) ê´€ê³„ í˜•ì„±
        for pair in combinations(tokens, 2):
            edges.append(tuple(sorted(pair)))

    # ìƒìœ„ Nê°œ í‚¤ì›Œë“œ ì¶”ì¶œ
    top_nodes = [node for node, count in node_counter.most_common(top_n_nodes)]
    
    # ìƒìœ„ Mê°œ ì—°ê²° ê´€ê³„ ì¶”ì¶œ
    edge_counts = Counter(edges).most_common(top_n_edges)
    
    # JSON êµ¬ì¡° ìƒì„± (DBì˜ 'graph_data' ì»¬ëŸ¼ì— ë“¤ì–´ê°ˆ ë°ì´í„°)
    network_data = {
        "nodes": [{"id": node, "count": node_counter[node]} for node in top_nodes],
        "links": [{"source": u, "target": v, "weight": w} 
                  for (u, v), w in edge_counts 
                  if u in top_nodes and v in top_nodes]
    }
    
    # í‚¤ì›Œë“œ ë¦¬ìŠ¤íŠ¸ (ë¬¸ìì—´ ë°°ì—´ í˜•íƒœ)
    keyword_list = top_nodes[:10]
    
    return json.dumps(network_data, ensure_ascii=False), keyword_list

# ==========================================
# 3. Gemini ì œëª© ìƒì„±
# ==========================================
def generate_title_with_gemini(titles):
    try:
        model = genai.GenerativeModel('gemini-2.0-flash-exp')
        prompt = f"""
        ë‹¤ìŒì€ ë™ì¼í•œ ë‰´ìŠ¤ ì‚¬ê±´ì— ëŒ€í•œ ê¸°ì‚¬ ì œëª©ë“¤ì…ë‹ˆë‹¤:
        {titles[:10]} (ì´ {len(titles)}ê±´)

        ì´ ë‰´ìŠ¤ë“¤ì„ ëª¨ë‘ í¬ê´„í•˜ëŠ” **í•˜ë‚˜ì˜ ê°„ê²°í•˜ê³  ì¤‘ë¦½ì ì¸ ì´ìŠˆ ì œëª©**ì„ ì‘ì„±í•´ì£¼ì„¸ìš”.
        
        [ì‘ì„± ê·œì¹™]
        1. 15ì ì´ë‚´ë¡œ ì§§ê²Œ ì‘ì„±í•  ê²ƒ.
        2. ì£¼ê´€ì ì´ê±°ë‚˜ ìê·¹ì ì¸ í‘œí˜„ì„ ë°°ì œí•  ê²ƒ (ì¤‘ë¦½ì  ì–´ì¡°).
        3. '~ë…¼ë€', '~ë°œí‘œ', '~ê°œìµœ' ë“± ëª…ì‚¬í˜•ìœ¼ë¡œ ëë§ºì„ ê²ƒ.
        4. ë”°ì˜´í‘œë‚˜ ì„¤ëª… ì—†ì´ ì˜¤ì§ ì œëª© í…ìŠ¤íŠ¸ë§Œ ì¶œë ¥í•  ê²ƒ.
        """
        response = model.generate_content(prompt)
        return response.text.strip().replace('"', '').replace("'", "")
    except Exception as e:
        print(f"   âš ï¸ Gemini í˜¸ì¶œ ì‹¤íŒ¨: {e}")
        return titles[0]

# ==========================================
# 4. ë©”ì¸ ë¶„ì„ ë¡œì§
# ==========================================
def analyze_weekly_top10(csv_path):
    print("ğŸ“¥ ë°ì´í„° ë¡œë”© ì¤‘...")
    df = pd.read_csv(csv_path)
    
    # 1. ì¤‘ë³µ ì œê±°
    df_clean = remove_duplicates_fast(df)
    
    print("ğŸš€ BERTopic í•™ìŠµ ì‹œì‘ (Full Analysis Mode)...")
    
    # ë¶ˆìš©ì–´ ì„¤ì •
    korean_stopwords = [
        "ë‰´ìŠ¤", "ì¢…í•©", "ì†ë³´", "ê¸°ì", "íŠ¹íŒŒì›", "ìœ„í•´", "ë°í˜”ë‹¤", "ëŒ€í•´", "ê´€ë ¨", 
        "ì˜¤ëŠ˜", "ì˜¤í›„", "ì˜¤ì „", "ê²ƒìœ¼ë¡œ", "ë”°ë¥´ë©´", "ìˆëŠ”", "í–ˆë‹¤", "ë§í–ˆë‹¤",
        "ë¯¼ì£¼ë‹¹", "êµ­ë¯¼ì˜í˜", "ì˜ì›", "ëŒ€í†µë ¹", "ëŒ€í‘œ"
    ]
    vectorizer = CountVectorizer(stop_words=korean_stopwords)
    
    # êµ°ì§‘í™” ëª¨ë¸ ì„¤ì •
    hdbscan_model = HDBSCAN(min_cluster_size=7, min_samples=3, prediction_data=True)
    
    topic_model = BERTopic(
        embedding_model="snunlp/KR-SBERT-V40K-klueNLI-augSTS",
        vectorizer_model=vectorizer,
        hdbscan_model=hdbscan_model,   
        nr_topics="auto",
        min_topic_size=7,
        calculate_probabilities=True,
        verbose=True
    )
    
    # ì œëª© ê°€ì¤‘ì¹˜ ê°•í™”
    docs = [str(t) + " " + str(t) + " " + str(t) + " " + str(c)[:100] 
            for t, c in zip(df_clean['title'], df_clean['content'])]
            
    topics, probs = topic_model.fit_transform(docs)
    
    df_clean['topic_id'] = topics
    if probs is not None and len(probs.shape) > 1:
        df_clean['prob'] = np.max(probs, axis=1)
    else:
        df_clean['prob'] = 1.0

    print("\nğŸ¤– ì´ìŠˆ ë¶„ì„ ë° í‚¤ì›Œë“œ ì¶”ì¶œ ì¤‘...")
    
    topic_info = topic_model.get_topic_info()
    top_topics = topic_info[topic_info['Topic'] != -1].head(15)
    
    final_results = []
    
    print(f"\nğŸ† ìµœì¢… ì´ìŠˆ ë¦¬ìŠ¤íŠ¸ (ì œëª© + í‚¤ì›Œë“œ + ê·¸ë˜í”„):")
    
    for idx, row in top_topics.iterrows():
        topic_id = row['Topic']
        count = row['Count']
        
        if count < 7: continue

        # í•´ë‹¹ ì´ìŠˆì˜ ê¸°ì‚¬ë“¤ ì¶”ì¶œ
        topic_indices = df_clean[df_clean['topic_id'] == topic_id].index
        topic_articles = df_clean.loc[topic_indices]
        topic_titles = topic_articles['title'].tolist()
        
        # 1) Gemini ì œëª© ìƒì„±
        time.sleep(1.0) 
        ai_label = generate_title_with_gemini(topic_titles)
        
        # 2) [NEW] í‚¤ì›Œë“œ ë„¤íŠ¸ì›Œí¬ ë°ì´í„° ìƒì„± (JSON)
        # ì œëª©ê³¼ ë³¸ë¬¸ì„ í•©ì³ì„œ ë¶„ì„ í…ìŠ¤íŠ¸ ì¤€ë¹„
        analysis_texts = (topic_articles['title'] + " " + topic_articles['content'].fillna('')).tolist()
        graph_json, keyword_list = extract_issue_network(analysis_texts)
            
        print(f"   [{idx+1}ìœ„] {ai_label} (ê¸°ì‚¬ {count}ê±´)")
        print(f"       ã„´ í•µì‹¬ í‚¤ì›Œë“œ: {', '.join(keyword_list[:5])}...")
        
        # ëŒ€í‘œ ê¸°ì‚¬ ì¶”ì¶œ (ìƒìœ„ 10ê°œë§Œ)
        representative_docs = topic_articles.sort_values(by='prob', ascending=False).head(10)
        
        for rank, (_, article) in enumerate(representative_docs.iterrows(), 1):
            final_results.append({
                "issue_rank": idx + 1,
                "issue_label": ai_label,       # AI ì œëª©
                "keywords": ",".join(keyword_list), # í‚¤ì›Œë“œ (ì½¤ë§ˆë¡œ êµ¬ë¶„ëœ ë¬¸ìì—´)
                "graph_data": graph_json,      # ì§€ì‹ ê·¸ë˜í”„ìš© JSON ë°ì´í„°
                "total_count": count,
                "article_rank": rank,
                "title": article['title'],
                "press": article['press'],
                "pub_date": article['pub_date'],
                "link": article['link'],
                "image_url": article.get('image_url', '') # ì´ë¯¸ì§€ URL ìˆìœ¼ë©´ ì €ì¥
            })

    if final_results:
        result_df = pd.DataFrame(final_results)
        # CSV íŒŒì¼ëª… ì„¤ì •
        filename = "weekly_top_issues_complete.csv"
        result_df.to_csv(filename, index=False, encoding="utf-8-sig")
        print(f"\nğŸ‰ ì €ì¥ ì™„ë£Œ! '{filename}' íŒŒì¼ì„ í™•ì¸í•˜ì„¸ìš”.")
        
        # JSON ë°ì´í„° ìƒ˜í”Œ ì¶œë ¥ (ë””ë²„ê¹…ìš©)
        print("\n[Sample Graph JSON Data - 1ìœ„ ì´ìŠˆ]")
        print(result_df.iloc[0]['graph_data'][:200] + "...") 
    else:
        print("âš ï¸ ì¶”ì¶œëœ ì´ìŠˆê°€ ì—†ìŠµë‹ˆë‹¤.")

if __name__ == "__main__":
    analyze_weekly_top10("weekly_politics_news_clean.csv")