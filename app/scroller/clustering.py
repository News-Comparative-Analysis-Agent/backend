import sys
import os

BASE_DIR = os.path.dirname(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))
sys.path.append(BASE_DIR)

from dotenv import load_dotenv
load_dotenv(os.path.join(BASE_DIR, ".env"))

import pandas as pd
import numpy as np
from bertopic import BERTopic
from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer
from sklearn.metrics.pairwise import cosine_similarity
from hdbscan import HDBSCAN
from umap import UMAP
import google.generativeai as genai
import time
import re
import json
from itertools import combinations
from collections import Counter
from konlpy.tag import Okt
from datetime import datetime
from app.core.database import SessionLocal, Base, engine
from app.domains.issues.models import IssueLabel
from app.domains.articles.models import Article, ArticleBody
from app.domains.topics.models import Topic
from app.domains.publishers.models import Publisher
from app.domains.keywordrelation.models import KeywordRelation

GOOGLE_API_KEY = os.getenv("GOOGLE_API_KEY") # .env íŒŒì¼ì—ì„œ ë¡œë“œ
genai.configure(api_key=GOOGLE_API_KEY)


def remove_duplicates_fast(df, threshold=0.90):
    if df.empty: return df
    df = df.reset_index(drop=True)
    
    print(f"ğŸ§¹ ì¤‘ë³µ ì œê±° ì „: {len(df)}ê°œ")
    tfidf = TfidfVectorizer(max_features=1000).fit_transform(
        df['content'].str[:300].fillna('')
    )
    duplicates = set()
    batch_size = 500
    num_docs = len(df)
    
    for i in range(0, num_docs, batch_size):
        batch_end = min(i + batch_size, num_docs)
        similarities = cosine_similarity(tfidf[i:batch_end], tfidf)
        for local_idx in range(batch_end - i):
            global_idx = i + local_idx
            if global_idx in duplicates: continue
            target_indices = np.where(similarities[local_idx, global_idx+1:] > threshold)[0]
            duplicates.update(target_indices + (global_idx + 1))
            
    df_clean = df.drop(index=list(duplicates)).reset_index(drop=True)
    print(f"âœ¨ ì¤‘ë³µ ì œê±° ì™„ë£Œ: {len(df_clean)}ê°œ")
    return df_clean

def simple_tokenizer(text):
    """ 
    Okt í˜•íƒœì†Œ ë¶„ì„ê¸°ë¥¼ ì‚¬ìš©í•˜ì—¬ 'ëª…ì‚¬'ë§Œ ì¶”ì¶œí•©ë‹ˆë‹¤. 
    'ë‹¨ì‹ì„' -> 'ë‹¨ì‹', 'ëŒ€í‘œëŠ”' -> 'ëŒ€í‘œ' ë¡œ ê¹”ë”í•˜ê²Œ ë³€í™˜ë©ë‹ˆë‹¤.
    """
    okt = Okt()
    
    # 1. ë¶ˆìš©ì–´ ë¦¬ìŠ¤íŠ¸ (ê³„ì† ì¶”ê°€í•´ì„œ ê´€ë¦¬í•˜ë©´ ì¢‹ìŠµë‹ˆë‹¤)
    stopwords = [
        'ë‰´ìŠ¤', 'ì¢…í•©', 'ì†ë³´', 'ê¸°ì', 'íŠ¹íŒŒì›', 'ìœ„í•´', 'ë°í˜”ë‹¤', 'ëŒ€í•´', 'ê´€ë ¨', 
        'ì˜¤ëŠ˜', 'ì˜¤í›„', 'ì˜¤ì „', 'ê²ƒìœ¼ë¡œ', 'ë”°ë¥´ë©´', 'ìˆëŠ”', 'í–ˆë‹¤', 'ë§í–ˆë‹¤',
        'ë¯¼ì£¼ë‹¹', 'êµ­ë¯¼ì˜í˜', 'ì˜ì›', 'ëŒ€í†µë ¹', 'ëŒ€í‘œ', 'ë¬´ë‹¨ì „ì¬', 'ë°°í¬', 'ê¸ˆì§€',
        'ì´ë‚ ', 'ì–´ì œ', 'ë‚´ì¼', 'ì´ë²ˆ', 'ì§€ë‚œ', 'ê°€ì¥', 'í†µí•´', 'ë•Œë¬¸', 'ê²½ìš°', 
        'ì •ë„', 'ì‚¬ì‹¤', 'ë‚´ìš©', 'ëª¨ë‘', 'ìš°ë¦¬', 'ìì‹ ', 'ë¬¸ì œ', 'ìƒê°', 'ì‚¬ëŒ',
        'ê·¸', 'ì´', 'ì €', 'ìˆ˜', 'ê²ƒ', 'ë“±', 'ì•ˆ', 'ì „', 'í›„', 'ì•½', 'ì¤‘'
    ]
    
    nouns = okt.nouns(str(text))
    filtered_nouns = [n for n in nouns if n not in stopwords and len(n) >= 2]
    
    return filtered_nouns


def extract_issue_network(texts, top_n_nodes=20, top_n_edges=30):
    """
    íŠ¹ì • ì´ìŠˆì— ì†í•œ ê¸°ì‚¬ í…ìŠ¤íŠ¸ë“¤ì„ ë°›ì•„ 'í‚¤ì›Œë“œ ë„¤íŠ¸ì›Œí¬ JSON'ì„ ìƒì„±í•©ë‹ˆë‹¤.
    """
    edges = []
    node_counter = Counter()

    for text in texts:
        
        tokens = list(set(simple_tokenizer(text)))
        node_counter.update(tokens)
        
        for pair in combinations(tokens, 2):
            edges.append(tuple(sorted(pair)))

    top_nodes = [node for node, count in node_counter.most_common(top_n_nodes)]
    
    edge_counts = Counter(edges).most_common(top_n_edges)
    
    network_data = {
        "nodes": [{"id": node, "count": node_counter[node]} for node in top_nodes],
        "links": [{"source": u, "target": v, "weight": w} 
                  for (u, v), w in edge_counts 
                  if u in top_nodes and v in top_nodes]
    }
    
    keyword_list = top_nodes[:10]
    
    return json.dumps(network_data, ensure_ascii=False), keyword_list, edge_counts

def generate_title_with_gemini(titles):
    try:
        model = genai.GenerativeModel('gemini-2.0-flash')
        prompt = f"""
        ë‹¤ìŒì€ ë™ì¼í•œ ë‰´ìŠ¤ ì‚¬ê±´ì— ëŒ€í•œ ê¸°ì‚¬ ì œëª©ë“¤ì…ë‹ˆë‹¤:
        {titles[:10]} (ì´ {len(titles)}ê±´)

        ì´ ë‰´ìŠ¤ë“¤ì„ ëª¨ë‘ í¬ê´„í•˜ëŠ” **í•˜ë‚˜ì˜ ê°„ê²°í•˜ê³  ì¤‘ë¦½ì ì¸ ì´ìŠˆ ì œëª©**ì„ ì‘ì„±í•´ì£¼ì„¸ìš”.
        
        [ì‘ì„± ê·œì¹™]
        1. 15ì ì´ë‚´ë¡œ ì§§ê²Œ ì‘ì„±í•  ê²ƒ.
        2. ì£¼ê´€ì ì´ê±°ë‚˜ ìê·¹ì ì¸ í‘œí˜„ì„ ë°°ì œí•  ê²ƒ (ì¤‘ë¦½ì  ì–´ì¡°).
        3. '~ë…¼ë€', '~ë°œí‘œ', '~ê°œìµœ' ë“± ëª…ì‚¬í˜•ìœ¼ë¡œ ëë§ºì„ ê²ƒ.
        4. ë”°ì˜´í‘œë‚˜ ì„¤ëª… ì—†ì´ ì˜¤ì§ ì œëª© í…ìŠ¤íŠ¸ë§Œ ì¶œë ¥í•  ê²ƒ.
        """
        response = model.generate_content(prompt)
        return response.text.strip().replace('"', '').replace("'", "")
    except Exception as e:
        print(f"   âš ï¸ Gemini í˜¸ì¶œ ì‹¤íŒ¨: {e}")
        return titles[0]

def save_to_db(df_articles, top_topics, keyword_data_map):
    """
    ë¶„ì„ëœ ì´ìŠˆ, ê¸°ì‚¬, í‚¤ì›Œë“œ ê´€ê³„ë¥¼ DBì— ì €ì¥í•©ë‹ˆë‹¤.
    keyword_data_map: topic_id -> (graph_json, keyword_list, edge_counts) ë§¤í•‘
    """
    print("\nğŸ’¾ ë°ì´í„°ë² ì´ìŠ¤ ì €ì¥ ì‹œì‘...")
    
    Base.metadata.create_all(bind=engine)
    
    db = SessionLocal()
    saved_issue_count = 0
    
    try:
        
        topic_name = "ì •ì¹˜"
        topic = db.query(Topic).filter(Topic.topic == topic_name).first()
        if not topic:
            topic = Topic(topic=topic_name)
            db.add(topic)
            db.flush()
        
        for idx, row in top_topics.iterrows():
            topic_id = row['Topic']
            count = row['Count']
            
            if count < 7: continue
            
            ai_label = row.get('ai_label', f"ì´ìŠˆ_{idx+1}")
            graph_json, keyword_list, edge_counts = keyword_data_map.get(topic_id, ({}, [], []))
            
            issue = IssueLabel(
                name=ai_label,
                keyword=keyword_list,
                total_count=int(count),
                created_at=datetime.now()
            )
            db.add(issue)
            db.flush() 
            
            today = datetime.now().date()
            for (u, v), w in edge_counts:
                if u in keyword_list and v in keyword_list:
                    rel = KeywordRelation(
                        date=today,
                        issue_label_id=issue.id,
                        keyword_a=min(u, v), 
                        keyword_b=max(u, v),
                        frequency=w
                    )
                    db.add(rel)

            topic_indices = df_articles[df_articles['topic_id'] == topic_id].index
            topic_articles = df_articles.loc[topic_indices]
            topic_articles = topic_articles.sort_values(by='prob', ascending=False)
            
            for rank, (_, row_art) in enumerate(topic_articles.iterrows(), 1):
                press_name = row_art['press']
                publisher = db.query(Publisher).filter(Publisher.name == press_name).first()
                if not publisher:
                    publisher = Publisher(name=press_name, code=press_name) # codeê°€ ì—†ìœ¼ë©´ name ì‚¬ìš©
                    db.add(publisher)
                    db.flush()
                
                # 2. ê¸°ì‚¬(Article) ì¤‘ë³µ í™•ì¸ (URL ê¸°ì¤€)
                existing_article = db.query(Article).filter(Article.url == row_art['link']).first()
                if existing_article:
                    continue # ì´ë¯¸ ìˆìœ¼ë©´ ìŠ¤í‚µ
                
                article = Article(
                    topic_id=topic.id,
                    issue_label_id=issue.id,
                    publisher_id=publisher.id,
                    title=row_art['title'],
                    url=row_art['link'],
                    image_urls=[row_art['image_url']] if row_art.get('image_url') else [],
                    published_at=pd.to_datetime(row_art['pub_date']),
                    analyzed_at=datetime.now()
                )
                db.add(article)
                db.flush()
                
                # 3. ë³¸ë¬¸(ArticleBody) ì €ì¥
                # contentê°€ ë„ˆë¬´ ê¸¸ë©´ ìë¥´ê±°ë‚˜ ì²˜ë¦¬ (Postgres TEXTëŠ” 1GBê¹Œì§€ ê°€ëŠ¥í•˜ë¯€ë¡œ ê´œì°®ìŒ)
                body = ArticleBody(
                    article_id=article.id,
                    raw_content=row_art['content']
                )
                db.add(body)
                
            saved_issue_count += 1
            
        db.commit()
        print(f"ğŸ‰ DB ì €ì¥ ì™„ë£Œ! ì´ {saved_issue_count}ê°œì˜ ì´ìŠˆê°€ ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.")
        
    except Exception as e:
        db.rollback()
        import traceback
        print(f"âš ï¸ DB ì €ì¥ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
        traceback.print_exc()
        print(f"   ğŸ‘‰ ë¬¸ì œ ë°œìƒ êµ¬ê°„ ì¶”ì : issue_idx={saved_issue_count}")
    finally:
        db.close()


def analyze_weekly_top10(csv_path):
    print("ë°ì´í„° ë¡œë”© ì¤‘")
    df = pd.read_csv(csv_path)
    
    df_clean = remove_duplicates_fast(df)
    
    print("BERTopic í•™ìŠµ ì‹œì‘")
    
    korean_stopwords = [
        "ë‰´ìŠ¤", "ì¢…í•©", "ì†ë³´", "ê¸°ì", "íŠ¹íŒŒì›", "ìœ„í•´", "ë°í˜”ë‹¤", "ëŒ€í•´", "ê´€ë ¨", 
        "ì˜¤ëŠ˜", "ì˜¤í›„", "ì˜¤ì „", "ê²ƒìœ¼ë¡œ", "ë”°ë¥´ë©´", "ìˆëŠ”", "í–ˆë‹¤", "ë§í–ˆë‹¤",
        "ë¯¼ì£¼ë‹¹", "êµ­ë¯¼ì˜í˜", "ì˜ì›", "ëŒ€í†µë ¹", "ëŒ€í‘œ"
    ]
    vectorizer = CountVectorizer(stop_words=korean_stopwords)
    
    hdbscan_model = HDBSCAN(min_cluster_size=7, min_samples=3, prediction_data=True)
    
    topic_model = BERTopic(
        embedding_model="snunlp/KR-SBERT-V40K-klueNLI-augSTS",
        vectorizer_model=vectorizer,
        hdbscan_model=hdbscan_model,   
        nr_topics="auto",
        min_topic_size=7,
        calculate_probabilities=True,
        verbose=True
    )
    
    docs = [str(t) + " " + str(t) + " " + str(t) + " " + str(c)[:100] 
            for t, c in zip(df_clean['title'], df_clean['content'])]
            
    topics, probs = topic_model.fit_transform(docs)
    
    df_clean['topic_id'] = topics
    if probs is not None and len(probs.shape) > 1:
        df_clean['prob'] = np.max(probs, axis=1)
    else:
        df_clean['prob'] = 1.0

    print("\nì´ìŠˆ ë¶„ì„ ë° í‚¤ì›Œë“œ ì¶”ì¶œ ì¤‘...")
    
    topic_info = topic_model.get_topic_info()
    top_topics = topic_info[topic_info['Topic'] != -1].head(15).copy() # ë³µì‚¬ë³¸ ì‚¬ìš©
    
    keyword_data_map = {} 
    
    print(f"\nìµœì¢… ì´ìŠˆ ë¦¬ìŠ¤íŠ¸ ì¶”ë¡  ì¤‘:")
    
    for idx, row in top_topics.iterrows():
        topic_id = row['Topic']
        count = row['Count']
        
        if count < 7: continue

        # í•´ë‹¹ ì´ìŠˆì˜ ê¸°ì‚¬ë“¤ ì¶”ì¶œ
        topic_indices = df_clean[df_clean['topic_id'] == topic_id].index
        topic_articles = df_clean.loc[topic_indices]
        topic_titles = topic_articles['title'].tolist()
        
        # 1) Gemini ì œëª© ìƒì„±
        time.sleep(1.0) 
        ai_label = generate_title_with_gemini(topic_titles)
        top_topics.at[idx, 'ai_label'] = ai_label # DataFrameì— ì €ì¥
        
        # 2) [NEW] í‚¤ì›Œë“œ ë„¤íŠ¸ì›Œí¬ ë°ì´í„° ìƒì„± (JSON)
        # ì œëª©ê³¼ ë³¸ë¬¸ì„ í•©ì³ì„œ ë¶„ì„ í…ìŠ¤íŠ¸ ì¤€ë¹„
        analysis_texts = (topic_articles['title'] + " " + topic_articles['content'].fillna('')).tolist()
        graph_json, keyword_list, edge_counts = extract_issue_network(analysis_texts)
        
        keyword_data_map[topic_id] = (graph_json, keyword_list, edge_counts)
            
        print(f"   [{idx+1}ìœ„] {ai_label} (ê¸°ì‚¬ {count}ê±´)")
        print(f"       ã„´ í•µì‹¬ í‚¤ì›Œë“œ: {', '.join(keyword_list[:5])}...")
        
    # DB ì €ì¥ í˜¸ì¶œ
    save_to_db(df_clean, top_topics, keyword_data_map)

if __name__ == "__main__":
    analyze_weekly_top10("weekly_politics_news_clean.csv")