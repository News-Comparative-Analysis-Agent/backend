import pandas as pd
import numpy as np
from bertopic import BERTopic
from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer
from sklearn.metrics.pairwise import cosine_similarity
from hdbscan import HDBSCAN
from umap import UMAP
import google.generativeai as genai
import time

GOOGLE_API_KEY = "models/gemini-2.0-flash-exp"

genai.configure(api_key=GOOGLE_API_KEY)

def remove_duplicates_fast(df, threshold=0.90):
    if df.empty: return df
    df = df.reset_index(drop=True)
    
    print(f"   ğŸ§¹ ì¤‘ë³µ ì œê±° ì „: {len(df)}ê°œ")
    tfidf = TfidfVectorizer(max_features=1000).fit_transform(
        df['content'].str[:300].fillna('')
    )
    duplicates = set()
    batch_size = 500
    num_docs = len(df)
    
    for i in range(0, num_docs, batch_size):
        batch_end = min(i + batch_size, num_docs)
        similarities = cosine_similarity(tfidf[i:batch_end], tfidf)
        for local_idx in range(batch_end - i):
            global_idx = i + local_idx
            if global_idx in duplicates: continue
            target_indices = np.where(similarities[local_idx, global_idx+1:] > threshold)[0]
            duplicates.update(target_indices + (global_idx + 1))
            
    df_clean = df.drop(index=list(duplicates)).reset_index(drop=True)
    print(f"ì¤‘ë³µ ì œê±° ì™„ë£Œ: {len(df_clean)}ê°œ")
    return df_clean


def generate_title_with_gemini(titles):
    """
    ê¸°ì‚¬ ì œëª© ë¦¬ìŠ¤íŠ¸ë¥¼ ë°›ì•„ Geminiê°€ 'ê¹”ë”í•œ ì´ìŠˆ ì œëª©' í•˜ë‚˜ë¥¼ ì‘ëª…í•´ì¤ë‹ˆë‹¤.
    """
    try:
        model = genai.GenerativeModel('gemini-2.0-flash-exp')
        
        # í”„ë¡¬í”„íŠ¸ ì—”ì§€ë‹ˆì–´ë§ (í•µì‹¬!)
        prompt = f"""
        ë‹¤ìŒì€ ë™ì¼í•œ ë‰´ìŠ¤ ì‚¬ê±´ì— ëŒ€í•œ ê¸°ì‚¬ ì œëª©ë“¤ì…ë‹ˆë‹¤:
        {titles[:10]} (ì´ {len(titles)}ê±´)

        ì´ ë‰´ìŠ¤ë“¤ì„ ëª¨ë‘ í¬ê´„í•˜ëŠ” **í•˜ë‚˜ì˜ ê°„ê²°í•˜ê³  ì¤‘ë¦½ì ì¸ ì´ìŠˆ ì œëª©**ì„ ì‘ì„±í•´ì£¼ì„¸ìš”.
        
        [ì‘ì„± ê·œì¹™]
        1. 15ì ì´ë‚´ë¡œ ì§§ê²Œ ì‘ì„±í•  ê²ƒ.
        2. ì£¼ê´€ì ì´ê±°ë‚˜ ìê·¹ì ì¸ í‘œí˜„ì„ ë°°ì œí•  ê²ƒ (ì¤‘ë¦½ì  ì–´ì¡°).
        3. '~ë…¼ë€', '~ë°œí‘œ', '~ê°œìµœ' ë“± ëª…ì‚¬í˜•ìœ¼ë¡œ ëë§ºì„ ê²ƒ.
        4. ë”°ì˜´í‘œë‚˜ ì„¤ëª… ì—†ì´ ì˜¤ì§ ì œëª© í…ìŠ¤íŠ¸ë§Œ ì¶œë ¥í•  ê²ƒ.
        """
        
        response = model.generate_content(prompt)
        return response.text.strip().replace('"', '').replace("'", "")
    
    except Exception as e:
        print(f"   Gemini í˜¸ì¶œ ì‹¤íŒ¨ (ê¸°ë³¸ ì œëª© ì‚¬ìš©): {e}")
        
        return titles[0]


def analyze_weekly_top10(csv_path):
    df = pd.read_csv(csv_path)
    
    # 1. ì¤‘ë³µ ì œê±°
    df_clean = remove_duplicates_fast(df)
    
    print("BERTopic í•™ìŠµ ì‹œì‘ (Gemini ì‘ëª… ëª¨ë“œ)...")
    
    korean_stopwords = [
        "ë‰´ìŠ¤", "ì¢…í•©", "ì†ë³´", "ê¸°ì", "íŠ¹íŒŒì›", "ìœ„í•´", "ë°í˜”ë‹¤", "ëŒ€í•´", "ê´€ë ¨", 
        "ì˜¤ëŠ˜", "ì˜¤í›„", "ì˜¤ì „", "ê²ƒìœ¼ë¡œ", "ë”°ë¥´ë©´", "ìˆëŠ”", "í–ˆë‹¤", "ë§í–ˆë‹¤",
        "ë¯¼ì£¼ë‹¹", "êµ­ë¯¼ì˜í˜", "ì˜ì›", "ëŒ€í†µë ¹", "ëŒ€í‘œ"
    ]
    vectorizer = CountVectorizer(stop_words=korean_stopwords)
    
    # êµ°ì§‘í™” ì„¤ì • (ì„¸ë°€í•˜ê²Œ)
    hdbscan_model = HDBSCAN(min_cluster_size=7, min_samples=3, prediction_data=True)
    
    topic_model = BERTopic(
        embedding_model="snunlp/KR-SBERT-V40K-klueNLI-augSTS",
        vectorizer_model=vectorizer,
        hdbscan_model=hdbscan_model,   
        nr_topics="auto",
        min_topic_size=7,
        calculate_probabilities=True,
        verbose=True
    )
    
    # ì œëª© ê°€ì¤‘ì¹˜ 3ë°°
    docs = [str(t) + " " + str(t) + " " + str(t) + " " + str(c)[:100] 
            for t, c in zip(df_clean['title'], df_clean['content'])]
            
    topics, probs = topic_model.fit_transform(docs)
    
    df_clean['topic_id'] = topics
    if probs is not None and len(probs.shape) > 1:
        df_clean['prob'] = np.max(probs, axis=1)
    else:
        df_clean['prob'] = 1.0

    
    print("\n Geminiê°€ ì´ìŠˆ ì œëª©ì„ ì§“ê³  ìˆìŠµë‹ˆë‹¤... (ì ì‹œë§Œ ê¸°ë‹¤ë ¤ì£¼ì„¸ìš”)")
    
    topic_info = topic_model.get_topic_info()
    top_topics = topic_info[topic_info['Topic'] != -1].head(15)
    
    final_results = []
    
    print(f"\nğŸ† ìµœì¢… ì´ìŠˆ ë¦¬ìŠ¤íŠ¸ (AI ì‘ëª…):")
    
    for idx, row in top_topics.iterrows():
        topic_id = row['Topic']
        count = row['Count']
        
        if count < 7: continue

        topic_indices = df_clean[df_clean['topic_id'] == topic_id].index
        topic_titles = df_clean.loc[topic_indices, 'title'].tolist()
        
        
        time.sleep(1.0) 
        ai_label = generate_title_with_gemini(topic_titles)
            
        print(f"   [{idx+1}ìœ„] {ai_label} (ê¸°ì‚¬ {count}ê±´)")
        
        representative_docs = df_clean[df_clean['topic_id'] == topic_id].sort_values(
            by='prob', ascending=False
        ).head(10)
        
        for rank, (_, article) in enumerate(representative_docs.iterrows(), 1):
            final_results.append({
                "issue_rank": idx + 1,
                "issue_label": ai_label, # AIê°€ ì§€ì€ ì˜ˆìœ ì œëª©
                "total_count": count,
                "article_rank": rank,
                "title": article['title'],
                "press": article['press'],
                "pub_date": article['pub_date'],
                "link": article['link']
            })

    if final_results:
        result_df = pd.DataFrame(final_results)
        filename = "weekly_top_issues_ai.csv"
        result_df.to_csv(filename, index=False, encoding="utf-8-sig")
        print(f"\nğŸ‰ ì €ì¥ ì™„ë£Œ! '{filename}' íŒŒì¼ì„ í™•ì¸í•˜ì„¸ìš”.")
    else:
        print("ì¶”ì¶œëœ ì´ìŠˆê°€ ì—†ìŠµë‹ˆë‹¤.")

if __name__ == "__main__":
    analyze_weekly_top10("weekly_politics_news_clean.csv")