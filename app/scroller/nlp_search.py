import requests
import os
from dotenv import load_dotenv
import json
import google.generativeai as genai
from newspaper import Article
from datetime import datetime
from collections import Counter
import html
import re


# Load .env from backend root
BASE_DIR = os.path.dirname(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))
load_dotenv(os.path.join(BASE_DIR, ".env"))

NAVER_CLIENT_ID = os.getenv("NAVER_CLIENT_ID")
NAVER_CLIENT_SECRET = os.getenv("NAVER_CLIENT_SECRET")
GOOGLE_API_KEY = os.getenv("GOOGLE_API_KEY")

genai.configure(api_key=GOOGLE_API_KEY)

class NewsBriefingAgent:
    def __init__(self):
        self.headers = {
            "X-Naver-Client-Id": NAVER_CLIENT_ID,
            "X-Naver-Client-Secret": NAVER_CLIENT_SECRET
        }

    def search_naver(self, query, display=10):
        """ ÎÑ§Ïù¥Î≤Ñ Îâ¥Ïä§ Í≤ÄÏÉâ """
        url = "https://openapi.naver.com/v1/search/news.json"
        params = {"query": query, "display": display, "sort": "date"}
        try:
            res = requests.get(url, headers=self.headers, params=params)
            return res.json().get('items', []) if res.status_code == 200 else []
        except:
            return []

    def fetch_full_content(self, url):
        """ Í∏∞ÏÇ¨ Î≥∏Î¨∏ Ïä§ÌÅ¨ÎûòÌïë (ÏÉÅÏúÑ Í∏∞ÏÇ¨Ïö©) """
        try:
            article = Article(url, language='ko')
            article.download()
            article.parse()
            if len(article.text) < 50: return None
            return article.text
        except:
            return None

    def generate_briefing(self, query, articles_data):
        try:
            model = genai.GenerativeModel('gemini-2.0-flash')
            
            # AIÏóêÍ≤å Ï§Ñ Ïª®ÌÖçÏä§Ìä∏ Îç∞Ïù¥ÌÑ∞ Íµ¨ÏÑ±
            context_text = ""
            for i, art in enumerate(articles_data):
                # ÏÉÅÏúÑ 3Í∞úÎäî Î≥∏Î¨∏ Ï†ÑÏ≤¥, ÎÇòÎ®∏ÏßÄÎäî ÏöîÏïΩÎ≥∏Îßå Ï†úÍ≥µ (ÌÜ†ÌÅ∞ Ï†àÏïΩ Î∞è ÏÜçÎèÑ)
                content = art.get('full_text', art['description']) 
                context_text += f"[{i+1}] Ïñ∏Î°†ÏÇ¨: {art['source']} | Ï†úÎ™©: {art['title']}\nÎÇ¥Ïö©: {content[:1000]}\n\n"

            prompt = f"""
            ÎãπÏã†ÏùÄ Ï†ïÏπò/ÏÇ¨Ìöå Ïù¥Ïäà Ï†ÑÎ¨∏ Î∂ÑÏÑùÍ∞ÄÏûÖÎãàÎã§.
            ÏÇ¨Ïö©ÏûêÍ∞Ä ÏöîÏ≤≠Ìïú Í≤ÄÏÉâÏñ¥: "{query}"
            
            ÏïÑÎûò Ï†úÍ≥µÎêú {len(articles_data)}Í∞úÏùò Îâ¥Ïä§ Í∏∞ÏÇ¨Îì§ÏùÑ Ï¢ÖÌï©Ï†ÅÏúºÎ°ú Î∂ÑÏÑùÌïòÏó¨ 'Ïù¥Ïäà Î∏åÎ¶¨Ìïë Î≥¥Í≥†ÏÑú'Î•º ÏûëÏÑ±Ìï¥Ï£ºÏÑ∏Ïöî.
            
            [Î∂ÑÏÑù ÏßÄÏπ®]
            1. ÌäπÏ†ï Ïñ∏Î°†ÏÇ¨Ïùò ÏãúÍ∞ÅÏóê ÏπòÏö∞ÏπòÏßÄ ÎßêÍ≥†, Ï§ëÎ¶ΩÏ†ÅÏù∏ ÏûÖÏû•ÏóêÏÑú ÏÑúÏà†ÌïòÏã≠ÏãúÏò§.
            2. ÎÖºÎûÄÏù¥ ÏûàÎäî Ïù¥ÏäàÎùºÎ©¥ 'Ï∞¨ÏÑ±/Î∞òÎåÄ' ÎòêÎäî 'Ïó¨Îãπ/ÏïºÎãπ/Ï†ïÎ∂Ä'Ïùò ÏûÖÏû•ÏùÑ Íµ¨Î∂ÑÌïòÏó¨ Ï†ïÎ¶¨ÌïòÏã≠ÏãúÏò§.
            3. Í∞ÄÏû• Ï§ëÏöîÌïú ÌïµÏã¨ ÌùêÎ¶ÑÏùÑ 3Î¨∏Îã® Ïù¥ÎÇ¥Î°ú ÏöîÏïΩÌïòÏã≠ÏãúÏò§.

            [ÏûÖÎ†• Îç∞Ïù¥ÌÑ∞]
            {context_text}

            [Ï∂úÎ†• ÌòïÏãù (JSON)]
            {{
                "summary_content": "Ï¢ÖÌï©Ï†ÅÏù∏ ÏöîÏïΩ ÎÇ¥Ïö© (ÎßàÌÅ¨Îã§Ïö¥ ÌòïÏãù Í∞ÄÎä•, Ï§ÑÎ∞îÍøàÏùÄ \\n)",
                "keywords": ["ÌÇ§ÏõåÎìú1", "ÌÇ§ÏõåÎìú2", "ÌÇ§ÏõåÎìú3", "ÌÇ§ÏõåÎìú4", "ÌÇ§ÏõåÎìú5"]
            }}
            """
            
            response = model.generate_content(prompt)
            clean_text = response.text.strip().replace("```json", "").replace("```", "")
            return json.loads(clean_text)
            
        except Exception as e:
            print(f"‚ö†Ô∏è Î∏åÎ¶¨Ìïë ÏÉùÏÑ± Ïã§Ìå®: {e}")
            return None

    def clean_text(self, text):
        """ HTML ÌÉúÍ∑∏ Ï†úÍ±∞ Î∞è ÏóîÌã∞Ìã∞(&quot; Îì±) Î≥ÄÌôò """
        if not text: return ""
        
        # 1. HTML ÌÉúÍ∑∏ Ï†úÍ±∞ (<b>, </b> Îì±)
        clean = re.sub(r'<[^>]+>', '', text)
        
        # 2. HTML ÏóîÌã∞Ìã∞ Î≥ÄÌôò (&quot; -> ", &lt; -> < Îì±)
        clean = html.unescape(clean)
        
        return clean

    def get_press_name(self, link, original_link):
        """ URLÏóêÏÑú Ïñ∏Î°†ÏÇ¨ Ïù¥Î¶ÑÏùÑ Ï∞æÏïÑÎÇ¥Îäî Ìï®Ïàò """
        # Ï£ºÏöî Ïñ∏Î°†ÏÇ¨ ÎèÑÎ©îÏù∏ Îß§Ìïë ÌÖåÏù¥Î∏î
        PRESS_MAP = {
            "chosun.com": "Ï°∞ÏÑ†ÏùºÎ≥¥", "hani.co.kr": "ÌïúÍ≤®Î†à", "yna.co.kr": "Ïó∞Ìï©Îâ¥Ïä§",
            "khan.co.kr": "Í≤ΩÌñ•Ïã†Î¨∏", "donga.com": "ÎèôÏïÑÏùºÎ≥¥", "joongang.co.kr": "Ï§ëÏïôÏùºÎ≥¥",
            "kbs.co.kr": "KBS", "imbc.com": "MBC", "sbs.co.kr": "SBS", "jtbc": "JTBC",
            "mk.co.kr": "Îß§ÏùºÍ≤ΩÏ†ú", "hankyung.com": "ÌïúÍµ≠Í≤ΩÏ†ú", "edaily.co.kr": "Ïù¥Îç∞ÏùºÎ¶¨",
            "mt.co.kr": "Î®∏ÎãàÌà¨Îç∞Ïù¥", "newsis.com": "Îâ¥ÏãúÏä§", "news1.kr": "Îâ¥Ïä§1",
            "kmib.co.kr": "Íµ≠ÎØºÏùºÎ≥¥", "seoul.co.kr": "ÏÑúÏö∏Ïã†Î¨∏", "segye.com": "ÏÑ∏Í≥ÑÏùºÎ≥¥",
            "munhwa.com": "Î¨∏ÌôîÏùºÎ≥¥", "etnews.com": "Ï†ÑÏûêÏã†Î¨∏", "zdnet.co.kr": "ZDNet",
            "nocutnews.co.kr": "ÎÖ∏Ïª∑Îâ¥Ïä§", "ohmynews.com": "Ïò§ÎßàÏù¥Îâ¥Ïä§", "pressian.com": "ÌîÑÎ†àÏãúÏïà",
            "dailian.co.kr": "Îç∞ÏùºÎ¶¨Ïïà", "inews24.com": "ÏïÑÏù¥Îâ¥Ïä§24", "fnnews.com": "ÌååÏù¥ÎÇ∏ÏÖúÎâ¥Ïä§"
        }

        target_url = original_link if original_link else link
        
        for domain, name in PRESS_MAP.items():
            if domain in target_url:
                return name
        
        return "Í∏∞ÌÉÄ Ïñ∏Î°†ÏÇ¨" # Îß§Ìïë Î¶¨Ïä§Ìä∏Ïóê ÏóÜÎäî Í≤ΩÏö∞

    def run(self, user_query):
        print(f"üîç '{user_query}' Í¥ÄÎ†® Í∏∞ÏÇ¨ ÏàòÏßë Ï§ë...")
        
        # 1. Í≤ÄÏÉâ (15Í∞ú Í∞ÄÏ†∏Ïò¥)
        items = self.search_naver(user_query, display=15)
        if not items: 
            print("‚ùå ÎÑ§Ïù¥Î≤Ñ Í≤ÄÏÉâ Í≤∞Í≥ºÍ∞Ä ÏóÜÏäµÎãàÎã§.")
            return {"success": False}

        processed_articles = []
        source_counter = Counter()

        # 2. Îç∞Ïù¥ÌÑ∞ Í∞ÄÍ≥µ (ÏÉÅÏúÑ 3Í∞úÎßå Deep Dive)
        for idx, item in enumerate(items):
            # Ïñ∏Î°†ÏÇ¨Î™Ö ÌååÏã±
            press_name = self.get_press_name(item['link'], item.get('originallink'))

            art_data = {
                "title": self.clean_text(item['title']),
                "link": item['link'],
                "description": self.clean_text(item['description']),
                "pubDate": item['pubDate'],
                "source": press_name
            }

            # ÏÉÅÏúÑ 3Í∞úÎäî Î≥∏Î¨∏ Í∏ÅÏñ¥Ïò§Í∏∞
            if idx < 3:
                full_text = self.fetch_full_content(item['link'])
                if full_text:
                    art_data['full_text'] = full_text
            
            processed_articles.append(art_data)
            source_counter[press_name] += 1

        # 3. Ï¢ÖÌï© Î∏åÎ¶¨Ìïë ÏÉùÏÑ±
        print("ü§ñ AI Î∂ÑÏÑùÍ∞ÄÍ∞Ä Î≥¥Í≥†ÏÑúÎ•º ÏûëÏÑ± Ï§ëÏûÖÎãàÎã§...")
        briefing = self.generate_briefing(user_query, processed_articles)
        
        # Î∏åÎ¶¨Ìïë Ïã§Ìå® Ïãú ÏòàÏô∏ Ï≤òÎ¶¨
        if not briefing:
             return {
                "success": False,
                "message": "AI Î∏åÎ¶¨Ìïë ÏÉùÏÑ±Ïóê Ïã§Ìå®ÌñàÏäµÎãàÎã§."
            }

        # 4. Response Body Íµ¨ÏÑ±
        final_keywords = briefing.get('keywords', [])
        
        # Í∏∞ÏÇ¨ Î¶¨Ïä§Ìä∏ Ìè¨Îß∑ÌåÖ
        formatted_articles = []
        for idx, art in enumerate(processed_articles):
            # Îß§Ïπ≠ ÌÇ§ÏõåÎìú ÌôïÏù∏
            matched = [k for k in final_keywords if k in art['title'] or k in art['description']]
            
            formatted_articles.append({
                "id": f"news_{idx+1:03d}",
                "title": art['title'],
                "source": art['source'],
                "description": art['description'],
                "link": art['link'],
                "pubDate": art['pubDate'],
                "relevance_score": 0.0, # ÎÑ§Ïù¥Î≤Ñ APIÎäî Ï†êÏàòÎ•º ÏïàÏ£ºÎØÄÎ°ú Í∏∞Î≥∏Í∞í Ï≤òÎ¶¨ (ÌïÑÏöîÏãú Í≥ÑÏÇ∞ Î°úÏßÅ Ï∂îÍ∞Ä)
                "matching_keywords": matched
            })

        return {
            "success": True,
            "data": {
                "original_query": user_query,
                "generated_keywords": final_keywords,
                "ai_summary": briefing.get('summary_content', ''),
                "total_results": len(formatted_articles),
                "articles": formatted_articles,
                "by_source": dict(source_counter)
            }
        }

# ==========================================
# Ïã§Ìñâ
# ==========================================
if __name__ == "__main__":
    agent = NewsBriefingAgent()
    result = agent.run("Ïù¥Ïû¨Î™Ö ÎåÄÌÜµÎ†π Í∏∞ÏûêÌöåÍ≤¨")
    print(json.dumps(result, indent=2, ensure_ascii=False))