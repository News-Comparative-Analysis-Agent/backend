import requests
from bs4 import BeautifulSoup
import pandas as pd
import time
import random
from datetime import datetime, timedelta

# ==========================================
# [ì„¤ì •] ìˆ˜ì§‘ ëŒ€ìƒ
# ==========================================
TARGET_PRESS_DICT = {
    "í•œê²¨ë ˆ": "028", "ê²½í–¥ì‹ ë¬¸": "032", 
    "ì¡°ì„ ì¼ë³´": "023", "ë™ì•„ì¼ë³´": "020", "ì—°í•©ë‰´ìŠ¤": "001"
}

DAYS_TO_CRAWL = 7
# í•„í„°ë§ ë° ì¤‘ë³µ ì œê±°ë¥¼ ê³ ë ¤í•´ ë„‰ë„‰íˆ íƒìƒ‰
SCAN_LIMIT = 50 

# ==========================================
# 1. ìƒì„¸ ìˆ˜ì§‘ í•¨ìˆ˜ (ê¸°ì¡´ê³¼ ë™ì¼)
# ==========================================
def get_article_detail_with_section(url):
    headers = {"User-Agent": "Mozilla/5.0"}
    try:
        res = requests.get(url, headers=headers, timeout=5)
        soup = BeautifulSoup(res.text, 'html.parser')
        
        # 1. ì„¹ì…˜ í™•ì¸
        section = ""
        meta_section = soup.select_one('meta[property="article:section"]')
        if meta_section:
            section = meta_section['content']
        else:
            cat_tag = soup.select_one('.media_end_categorize_item')
            if cat_tag:
                section = cat_tag.get_text(strip=True)
        
        if section != "ì •ì¹˜":
            return None 
            
        # 2. ë³¸ë¬¸ ì¶”ì¶œ
        content_area = soup.select_one('#dic_area') or soup.select_one('#newsct_article')
        content = ""
        if content_area:
            for tag in content_area.select('.img_desc, .end_photo_org, .media_end_summary, .byline_s'):
                tag.extract()
            content = content_area.get_text(strip=True)
            
        # 3. ì´ë¯¸ì§€ & ë‚ ì§œ
        img_tag = soup.select_one('meta[property="og:image"]')
        image_url = img_tag['content'] if img_tag else ""
        
        date_tag = soup.select_one('.media_end_head_info_datestamp span')
        pub_date = date_tag['data-date-time'] if date_tag else ""

        return {
            "section": section,
            "content": content,
            "image_url": image_url,
            "pub_date": pub_date
        }
    except Exception:
        return None

# ==========================================
# 2. ë©”ì¸ í¬ë¡¤ëŸ¬ (ì¤‘ë³µ ì œê±° ë¡œì§ ì¶”ê°€)
# ==========================================
def crawl_unique_politics_news():
    all_news = []
    
    # ğŸ”¥ [í•µì‹¬] ì¤‘ë³µ ë°©ì§€ìš© 'ê¸°ì‚¬ ID' ì €ì¥ì†Œ
    # URLì´ë‚˜ ê¸°ì‚¬ ê³ ìœ  IDë¥¼ ì €ì¥í•´ë‘ê³ , ì´ë¯¸ ìˆìœ¼ë©´ ê±´ë„ˆëœë‹ˆë‹¤.
    seen_articles = set()
    
    today = datetime.now()
    headers = {"User-Agent": "Mozilla/5.0"}
    
    print(f"ğŸš€ ì •ì¹˜ ë‰´ìŠ¤ ìˆ˜ì§‘ ì‹œì‘ (ì¤‘ë³µ ì›ì²œ ì°¨ë‹¨)...\n")
    
    for day_offset in range(DAYS_TO_CRAWL):
        target_date = today - timedelta(days=day_offset)
        date_str = target_date.strftime("%Y%m%d")
        display_date = target_date.strftime("%Y-%m-%d")
        
        print(f"ğŸ“… [Day {day_offset+1}/{DAYS_TO_CRAWL}] {display_date} íƒìƒ‰ ì¤‘...")
        
        for press_name, oid in TARGET_PRESS_DICT.items():
            url = f"https://news.naver.com/main/ranking/office.naver?officeId={oid}&date={date_str}"
            
            try:
                res = requests.get(url, headers=headers)
                soup = BeautifulSoup(res.text, 'html.parser')
                list_items = soup.select('.rankingnews_list li')
                
                if not list_items: continue

                collected_count = 0
                for item in list_items:
                    # ì–¸ë¡ ì‚¬ë³„ í•˜ë£¨ 10ê°œë§Œ ì €ì¥
                    if collected_count >= 10: break 
                    
                    link_tag = item.select_one('a')
                    if not link_tag: continue
                    
                    link = link_tag['href']
                    if link.startswith("/"): link = "https://news.naver.com" + link
                    
                    # ğŸ’¡ URLì—ì„œ ê³ ìœ  ì‹ë³„ì(article id)ë§Œ ì¶”ì¶œí•´ì„œ ë¹„êµí•˜ë©´ ë” ì •í™•í•¨
                    # ì˜ˆ: https://n.news.naver.com/article/028/0002674384 -> '028/0002674384'
                    try:
                        article_id = link.split("/article/")[1]
                        # ?sid=... ê°™ì€ íŒŒë¼ë¯¸í„° ì œê±°
                        article_id = article_id.split("?")[0] 
                    except:
                        article_id = link # ì‹¤íŒ¨í•˜ë©´ ë§í¬ ì „ì²´ ì‚¬ìš©

                    # ğŸ”¥ [ì¤‘ë³µ ê²€ì‚¬] ì´ë¯¸ ìˆ˜ì§‘í•œ ê¸°ì‚¬ë©´ íŒ¨ìŠ¤!
                    if article_id in seen_articles:
                        continue
                        
                    # ìˆ˜ì§‘ ëª©ë¡ì— ë„ì¥ ì¾…!
                    seen_articles.add(article_id)
                    
                    title = link_tag.get_text(strip=True)
                    
                    # ìƒì„¸ í˜ì´ì§€ ì ‘ì† & ì •ì¹˜ ì—¬ë¶€ í™•ì¸
                    detail = get_article_detail_with_section(link)
                    
                    if detail and len(detail['content']) > 50:
                        all_news.append({
                            "collection_date": display_date,
                            "press": press_name,
                            "title": title,
                            "section": detail['section'],
                            "content": detail['content'],
                            "image_url": detail['image_url'],
                            "pub_date": detail['pub_date'],
                            "link": link
                        })
                        collected_count += 1
                    
                    time.sleep(random.uniform(0.05, 0.1))
                
                print(f"   âœ… {press_name}: ì‹ ê·œ {collected_count}ê°œ ì €ì¥")
                
            except Exception as e:
                print(f"   âš ï¸ {press_name} ì—ëŸ¬: {e}")
                
    return pd.DataFrame(all_news)

# ì‹¤í–‰
if __name__ == "__main__":
    df_unique = crawl_unique_politics_news()
    
    if not df_unique.empty:
        print(f"\nğŸ‰ ìˆ˜ì§‘ ì™„ë£Œ! ì´ {len(df_unique)}ê°œ")
        
        # ì¤‘ë³µì´ ì§„ì§œ ì—†ëŠ”ì§€ í™•ì¸
        print(f"ì¤‘ë³µ ì œê±° ì „: {len(df_unique) + (len(df_unique) - len(df_unique['link'].unique()))}") # ì˜ˆì‹œ ê³„ì‚°
        print(f"ì¤‘ë³µ ì œê±° í›„: {len(df_unique)}")
        
        filename = "weekly_politics_news_clean.csv"
        df_unique.to_csv(filename, index=False, encoding="utf-8-sig")
        print(f"ğŸ“ '{filename}'ì— ê¹”ë”í•˜ê²Œ ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.")
    else:
        print("ìˆ˜ì§‘ëœ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")